---
title: "Analysing the Russia Report"
author: "Andi Fugard (almost@gmail.com, @[inductivestep](https://twitter.com/InductiveStep))"
date: "`r format(Sys.time(), '%d %B %Y')`"
output:
  html_document:
    self_contained: no
    toc: yes
    toc_float: yes
    collapsed: false
---

(See the github repo [here](https://github.com/InductiveStep/Russia-Report).)

The UK’s Intelligence and Security Committee’s report into Russian activity in the UK was finally on the 21 July 2020.

The heavily-redacted text is available on the [ISC website](http://isc.independent.gov.uk/news-archive/21july2020) and [in this github repository](20200721_HC632_CCS001_CCS1019402408-001_ISC_Russia_Report_Web_Accessible.pdf).

I'm going to have a look with the aid of some tidyverse widgets.

First, load packages and the text into R.

```{r message=FALSE, warning=FALSE}
devtools::install_github("inductivestep/handbag")
library(handbag)
library(tidyverse)
library(knitr)
library(pdftools)
library(stringr)
rr_text <- pdf_text("20200721_HC632_CCS001_CCS1019402408-001_ISC_Russia_Report_Web_Accessible.pdf")
```

The `pdf_text` command returns a character vector with one element per PDF page. The main report text is on pages 8-42.


# First go

## Finding the juiciest pages

I'd like to know where the juicy pages are - maybe it's those with the most redactions, `***`?

Let's count redactions per page:

```{r}
redact_n <- str_count(rr_text, pattern = "\\*\\*\\*")
```

Now add these to a tibble, alongside the (pdf, as opposed to printed) page number and original page text for later analyses.

```{r}
rr_tib <- tibble(page = 1:length(rr_text),
                 text = rr_text,
                 redactions = redact_n)
```

### Histogram

A histogram of those redactions:

```{r fig.height=3, fig.width=6, message=FALSE}
ggplot(rr_tib, aes(x=redactions)) +
  geom_histogram(binwidth = 1, fill = "firebrick", color = "white") +
  labs(x = "Number of redactions",
       y = "Count",
       title = "Redactions per page in the Russia Report") +
  theme_classic()
```

### Which pages have the most redactions?

```{r}
rr_tib %>%
  filter(redactions >= 10) %>%
  select(page, redactions) %>%
  arrange(redactions) %>%
  kable()
```

The section on PDF pages 37 and 38 (printed page nums 30 and 31) is the winner: "Rising to the challenge". Here's a link to the [pdf pages](https://inductivestep.github.io/Russia-Report/20200721_HC632_CCS001_CCS1019402408-001_ISC_Russia_Report_Web_Accessible.pdf#page=37).


## Word counts

For this, I'll use the `tidytext` package, which is introduced in a [great book](https://www.tidytextmining.com/) by [Julia Silge](https://juliasilge.com/) and [David Robinson](http://varianceexplained.org/).

```{r}
library(tidytext)
```

Here's a tidy tibble with all the words, using `unnest_tokens`:

```{r}
rr_words <- rr_tib %>%
  select(page,text) %>%
  unnest_tokens(word, # name of the new column
                text) # column with text

```

The top 20 most common words, excluding [stop words](https://en.wikipedia.org/wiki/Stop_words), are:

```{r fig.height=6, fig.width=6}
rr_words %>%
  anti_join(stop_words, by = "word") %>%
  count(word) %>%
  arrange(desc(n)) %>%
  head(20) %>%
  ggplot(aes(x = reorder(word, n),
             y = n,
             fill = n)) +
    geom_bar(stat = "identity") + 
    coord_flip() +
    labs(y = "Count",
         x = "Word") + 
    theme(legend.position = "none")
```

It might also be fun to find initialisms like MI5, etc. - we can do that with a regular expression.

The previous scan of all words reduced them to lower case, so let's go again using `unnest_tokens` with `to_lower = FALSE`:

```{r}
initialism_regex <- "([A-Z]{2})|(([A-Z].){2})"

rr_initialisms <- rr_tib %>%
  select(page,text) %>%
  unnest_tokens(word,
                text,
                to_lower = FALSE) %>%
  filter(str_detect(word, initialism_regex))

head(rr_initialisms)
```

It's apparent that this picks up too many, e.g., there are some upper case words like "INTELLIGENCE". I've selected a few interesting actual initialisms for later.

```{r}
rr_initialisms %>%
  group_by(word) %>%
  count() %>%
  arrange(desc(n)) %>%
  kable()
```



## Sentence-level analysis

Is there a relationship between which intelligence service (e.g., MI5, MI6, or GCHQ) is mentioned and whether there is a redaction in that sentence?

Again use `tidytext`, but this time at the sentence level.

```{r}
rr_sentences <- rr_tib %>%
  unnest_sentences(sentence, text,
                   to_lower = FALSE) %>%
  select(-redactions) # remove page-level count of redactions

rr_sentences$sentence_i <- 1:nrow(rr_sentences)
```

That worked reasonably well, though:

* Citations at the end of sentences confused it (possible to fix this by using `unnest_regex`).
* Footnotes need to be addressed separately (they are sentences beginning with numbers, once the aforementioned sentence problem has been fixed).
* It doesn't deal with sentences which cross pages - definitely not when there's a footnote splitting them too (it may be possible to fix this problem by removing footnotes, gluing all the sentences together, and unnesting again).

Next: code redactions and whether sentences mention one of the big three intel agencies (and some other organisations, for comparison).

I'll use (the fantastic!) `fuzzyjoin` for this, which joins two tables based on a regular expression.

```{r}
library(fuzzyjoin)
```

Here's the mapping:

```{r}
mapping <- tribble(
  ~regex,                                ~code,
  "MI5|[s|S]ecurity [s|S]ervice",        "org_MI5",
  "MI6|SIS|Secret Intelligence Service", "org_MI6",
  "GCHQ",                                "org_GCHQ",
  "Defence Intelligence|DI",             "org_DI",
  "GRU",                                 "org_GRU",
  "NATO",                                "org_NATO",
  "RIS",                                 "org_RIS",
  "NCSC",                                "org_NCSC",
  "NSS",                                 "org_NSS",
  "OSCT",                                "org_OSCT",
  "ISC",                                 "org_ISC",
  "FCO",                                 "org_FCO",
  "UN",                                  "org_UN",
  "RAF",                                 "org_RAF",
  "JIC",                                 "org_JIC",
  "DCMS",                                "org_DCMS",
  initialism_regex,                      "initialism",
  "\\*\\*\\*",                           "redacted"
)
```

Now, fuzzyjoin and reshape so that there's a binary variable for each feature:

```{r}
rr_mentions <- rr_sentences %>%
  regex_left_join(mapping,
                  by = c(sentence = "regex")) %>%
  group_by(sentence_i, code) %>%
  count() %>%
  pivot_wider(names_from = code,
              values_from = n,
              values_fill = 0) %>%
  rowwise() %>%
  mutate(any_org = 
           max(c_across(starts_with("org_")))) %>%
  select(-"NA")

rr_sentences <- rr_sentences %>%
  left_join(rr_mentions, by = "sentence_i")
```

Make a summary variable of the services mentioned.

```{r}
rr_sentences$mentioned_services <- rr_sentences %>%
  select(starts_with("org_")) %>%
  handbag::binary_patterns_var(strip_prefix = "org_")
```

Let's count them:

```{r}
rr_sentences %>%
  count(mentioned_services) %>%
  arrange(desc(n)) %>%
  kable(col.names = c("Service(s) mentioned", "n"))
```

Does mention of any of these organisations in a sentence predict a redaction in the same sentence?

```{r message=FALSE}
redacted_tab <- rr_sentences %>%
  group_by(mentioned_services, redacted) %>%
  summarise(n = n()) %>%
  mutate(perc = 100*n/sum(n)) %>%
  pivot_wider(names_from   = redacted,
              values_from  = c(n,perc),
              values_fill  = 0,
              names_prefix = "redact_")

redacted_tab %>%
  select(-perc_redact_0) %>%
  arrange(desc(perc_redact_1)) %>%
  kable(col.names = c("", "Unredacted (n)",
                      "Redacted (n)",
                      "Redacted (%)"),
        digits = 0)
```

As a picture:

```{r fig.height=12, fig.width=6, message=FALSE}
redacted_tab %>%
  mutate(percentage_redacted = perc_redact_1) %>%
  ggplot(aes(x = reorder(mentioned_services, percentage_redacted),
             y = percentage_redacted,
             fill = mentioned_services)) +
    geom_col() + 
    coord_flip() +
    labs(x = "Services mentioned in sentence",
         y = "% of sentences with a redaction",
         title = "Predictors of redacted sentences",
         caption =
           "Bars are labelled with counts (redactions/total)") + 
    expand_limits(y = 105) +
    theme(legend.position = "none") +
    geom_text(aes(label = paste0(n_redact_1,"/",
                                 n_redact_1+n_redact_0),
                  y = percentage_redacted+1,
                  hjust = 0),
              size = 4)
```

(The winner there, NSS mentioned alone, with 100% redactions is only two sentences.)



### View the text

First, a function to make it easier to view the text.

```{r}
viewSentences <- function(the_sentences) {
  the_sentences %>%
    mutate(nice_sentence = gsub(pattern = "\\*\\*\\*",
           replace = "\\*\\*\\*[redacted]\\*\\*\\*",
           x = sentence)
    ) %>%
    select(page, mentioned_services, nice_sentence) %>%
    kable(col.names = c("Page", "Services", "Sentence"))
}
```


**Note be careful interpreting the numbers - they're mostly superscript citations!**


#### Sentences mentioning an organisation and with a redaction

```{r}
rr_sentences %>%
  filter(any_org == 1 & redacted == 1) %>%
  viewSentences()
```

#### Sentences mentioning the organisations but with no redaction

```{r}
rr_sentences %>%
  filter(any_org == 1 & redacted == 0) %>%
  viewSentences()
```




#### Sentences mentioning none of the organisations but still redacted

```{r}
rr_sentences %>%
  filter(any_org == 0 & redacted == 1) %>%
  viewSentences()
```


## Sentiment analysis

Here's the plan:

1. Take the sentence level analysis from above.
2. Unnest further to the word level.
3. Explore some of relationships with sentiment, using the word-emotion lexicon by [Saif M. Mohammad and Peter D. Turney (2013)](https://doi.org/10.1111/j.1467-8640.2012.00460.x):

```{r}
library(textdata)
nrc_sentiments <- get_sentiments("nrc")
```

```{r}
rr_sentiment <- rr_sentences %>%
  unnest_tokens(word,
                sentence, drop = F)

rr_sentiment$word_i <- 1:nrow(rr_sentiment)

rr_sentiment <- rr_sentiment %>%
  inner_join(nrc_sentiments, by = "word")
```

Let's see which report words appear by sentiment:

```{r}
library(ggwordcloud)
```

```{r message=FALSE, warning=FALSE}
makeWordCloud <- function(theSentiment) {
  rr_sentiment %>%
    filter(sentiment == theSentiment) %>%
    group_by(word) %>%
    summarise(n = n()) %>%
    mutate(prop = n/sum(n)) %>%
    filter(n >= 5) %>%
    ggplot(aes(label = word,
               size = log1p(prop),
               col = log1p(n))) +
    scale_size_area(max_size = 12) +
    geom_text_wordcloud() +
    theme_minimal() + 
    labs(title = theSentiment)
}

allTheClouds <- map(unique(rr_sentiment$sentiment)
                      %>% na.omit(), makeWordCloud)
allTheClouds
```



I'm not entirely convinced by the words picked out for each of those sentiments - maybe anger, disgust, and sadness show something interesting.

```{r}
toView <- c("anger", "disgust", "sadness")
```

First glue the word-level analysis back onto the sentences.

I want a count by sentence of each sentence (`n`) and also a count of whether a sentiment has been expressed at least once per sentence (the `b` below is for Boolean).

```{r}
sentiment_counts <- rr_sentiment %>%
  group_by(sentence_i, sentiment) %>%
  summarise(n = n()) %>%
  mutate(b = as.numeric(n > 0))

sentiment_counts <- rr_sentences %>%
  inner_join(sentiment_counts, by = "sentence_i")
```

Now plot by page how many times each sentiment has been expressed:

```{r fig.height=3, fig.width=6}
emo_by_page <- sentiment_counts %>%
  group_by(page, sentiment) %>%
  filter(sentiment != "NA") %>%
  summarise(instances = sum(n)) %>%
  mutate(p = instances/sum(instances))

emo_by_page %>%
  filter(sentiment %in% toView) %>%
  ggplot(aes(x = page, y = instances, color = sentiment)) +
    geom_line() +
    labs(x = "Page", y = "Count")
```

Looks like a spike for "anger" there on PDF [page 18](https://inductivestep.github.io/Russia-Report/20200721_HC632_CCS001_CCS1019402408-001_ISC_Russia_Report_Web_Accessible.pdf#page=18).

```{r}
emo_by_page %>%
  filter (instances > 20 & sentiment %in% toView) %>%
  kable(col.names = c("Page", "Sentiment", "N", "Proportion"))
```

```{r}
sentiment_counts %>%
  filter(page == 18 & sentiment %in% toView) %>%
  select(sentiment, sentence) %>%
  kable()
```




## Word cloud of redactions

Now we have data labelled in various ways, it's easy to make word clouds comparing words in sentences which have redactions with those which don't.

Firstly, let's just do this for all sentences:

```{r fig.height=3, fig.width=6, message=FALSE, warning=FALSE}
rr_sentiment %>%
  anti_join(stop_words) %>%
  mutate(redacted = recode(redacted,
                           `0` = "No redactions",
                           `1` = "One or more redactions")) %>%
  group_by(redacted, word) %>%
  summarise(n = n()) %>%
  mutate(prop = n/sum(n)) %>%
  filter(prop > .002) %>%
  ggplot(aes(label = word, size = log1p(prop), color = redacted)) +
    scale_size_area(max_size = 11) +
    geom_text_wordcloud() +
    theme_minimal() + 
    facet_grid(cols = vars(redacted))
```

Some words are irritating in the output - remove these:

```{r}
excluded_words <- c("gchq",
                    "mi6",
                    "mi5",
                    "sis",
                    "intelligence",
                    "www.mi5")
```


(Note below how stop words have been removed too with `anti_join`).

```{r fig.height=36, fig.width=6, message=FALSE, warning=FALSE}
rr_sentiment %>%
  anti_join(stop_words) %>%
  mutate(redacted = recode(redacted,
                           `0` = "No redactions",
                           `1` = "One or more redactions")) %>%
  group_by(redacted, mentioned_services, word) %>%
  summarise(n = n()) %>%
  mutate(prop = n/sum(n)) %>%
  filter(prop > .01 & !(word %in% excluded_words)) %>%
  mutate(services_wrapped = stringr::str_wrap(mentioned_services,5)) %>%
  ggplot(aes(label = word, size = log1p(prop), color = redacted)) +
    scale_size_area(max_size = 9) +
    geom_text_wordcloud() +
    theme_minimal() + 
    facet_grid(cols = vars(redacted),
               rows = vars(services_wrapped))
```

